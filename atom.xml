<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Light-white&#39;s blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://light-white.me/"/>
  <updated>2017-03-26T08:42:31.539Z</updated>
  <id>http://light-white.me/</id>
  
  <author>
    <name>Light-white</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用Scrapy抓取链家网租房信息</title>
    <link href="http://light-white.me/2017/03/15/%E4%BD%BF%E7%94%A8Scrapy%E6%8A%93%E5%8F%96%E9%93%BE%E5%AE%B6%E7%BD%91%E7%A7%9F%E6%88%BF%E4%BF%A1%E6%81%AF/"/>
    <id>http://light-white.me/2017/03/15/使用Scrapy抓取链家网租房信息/</id>
    <published>2017-03-15T05:54:13.000Z</published>
    <updated>2017-03-26T08:42:31.539Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Scrapy介绍"><a href="#1-Scrapy介绍" class="headerlink" title="1. Scrapy介绍"></a>1. Scrapy介绍</h1><p>Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。<br>Scrapy吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。<br>Scratch，是抓取的意思，这个Python的爬虫框架叫Scrapy，大概也是这个意思吧，就叫它：小刮刮吧。<br>Scrapy的官方网站：<a href="https://scrapy.org/" target="_blank" rel="external">https://scrapy.org/</a><br>关于Scrapy的安装可以看我之前的博客 <a href="http://light-white.me/2017/02/06/Windows%E4%B8%8BPython%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6Scrapy%E5%AE%89%E8%A3%85/">Windows下Python爬虫框架Scrapy安装</a>  </p>
<h1 id="2-创建爬虫项目"><a href="#2-创建爬虫项目" class="headerlink" title="2. 创建爬虫项目"></a>2. 创建爬虫项目</h1><p>在控制台执行<br><code>scrapy startproject myspider</code><br>目录结构如下:  </p>
<pre><code>└─myspider
    │  scrapy.cfg
    └─myspider
        │  items.py
        │  middlewares.py
        │  pipelines.py
        │  settings.py
        │  __init__.py
        ├─spiders
        │  │  __init__.py
        │  └─__pycache__
        └─__pycache__
</code></pre><h1 id="3-编写爬虫"><a href="#3-编写爬虫" class="headerlink" title="3.编写爬虫"></a>3.编写爬虫</h1><p>在spiders目录下 新建<code>lianjia_spiders.py</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"><span class="keyword">import</span> re</div><div class="line"></div><div class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</div><div class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> Request</div><div class="line"><span class="keyword">from</span> lianjia.items <span class="keyword">import</span> LianjiaItem</div><div class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</div><div class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LianjiaSpider</span><span class="params">(CrawlSpider)</span>:</span></div><div class="line">    name = <span class="string">'lianjia'</span></div><div class="line">    allowed_domains = [<span class="string">'lianjia.com'</span>]</div><div class="line">    rules = (</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">r'^http://bj.lianjia.com/zufang/BJ[0-9]&#123;10&#125;'</span>, )), callback=<span class="string">'parse_item'</span>,follow=<span class="keyword">True</span>),</div><div class="line">        Rule(LinkExtractor(allow=(<span class="string">r'^http://bj.lianjia.com/zufang/[0-9]&#123;12&#125;'</span>, )), callback=<span class="string">'parse_item'</span>,follow=<span class="keyword">True</span>),</div><div class="line">    )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">yield</span> scrapy.Request(<span class="string">'http://bj.lianjia.com/zufang/'</span>)</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">101</span>):</div><div class="line">            <span class="keyword">yield</span> scrapy.Request(<span class="string">'http://bj.lianjia.com/zufang/pg%d/'</span>%i)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span><span class="params">(self, response)</span>:</span></div><div class="line">        print(<span class="string">"-----parse_item-----"</span>)</div><div class="line">        selector = Selector(response)</div><div class="line">        item = LianjiaItem()</div><div class="line">        item[<span class="string">'title'</span>] = selector.xpath(<span class="string">'/html/body/div[4]/div[1]/div/div[1]/h1/text()'</span>).extract_first()</div><div class="line">        item[<span class="string">'area'</span>] = selector.xpath(<span class="string">'/html/body/div[4]/div[2]/div[2]/div[2]/p[1]/text()'</span>).extract_first()</div><div class="line">        item[<span class="string">'price'</span>] = selector.xpath(<span class="string">'/html/body/div[4]/div[2]/div[2]/div[1]/span[1]/text()'</span>).extract_first()</div><div class="line">        regex = selector.xpath(<span class="string">'/html/body/script[12]/text()'</span>).extract()</div><div class="line">        position = re.search(<span class="string">'resblockPosition:(.+),'</span>,regex.pop())</div><div class="line">        item[<span class="string">'position'</span>] = position.group()[:<span class="number">-1</span>].split(<span class="string">':'</span>)[<span class="number">1</span>]</div><div class="line">        item[<span class="string">'url'</span>] = response.url</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure></p>
<p>其中我们要编写一个类继承<code>scrapy.spiders</code>，不过这里我使用的<code>CrawlSpider</code>来做全站爬取<br>其中<code>name</code>是用来标识一个爬虫，名字唯一<br><code>allowed_domains</code>是允许爬虫爬取的域<br>其中还应该包含一个<code>start_urls</code>作为爬虫爬取的初始页面<br>不过这里我使用<code>start_requests</code>来生成爬取列表了<br><code>rules</code>是<code>CrawlSpider</code>中的属性，用来匹配当前页面中的链接  </p>
<h1 id="4-items"><a href="#4-items" class="headerlink" title="4.items"></a>4.items</h1><p>items.py中我们编写要抓取的对象信息<br>每次爬取页面都会返回一个item<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> scrapy</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LianjiaItem</span><span class="params">(scrapy.Item)</span>:</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    id = scrapy.Field()        <span class="comment">#存入数据库的id</span></div><div class="line">    title = scrapy.Field()     <span class="comment">#房屋页面的标题</span></div><div class="line">    area = scrapy.Field()      <span class="comment">#房屋的面积</span></div><div class="line">    price = scrapy.Field()     <span class="comment">#每月房租</span></div><div class="line">    position = scrapy.Field()  <span class="comment">#房屋的经纬度信息</span></div><div class="line">    url = scrapy.Field()       <span class="comment">#网址</span></div></pre></td></tr></table></figure></p>
<h1 id="5-Selector选择器"><a href="#5-Selector选择器" class="headerlink" title="5.Selector选择器"></a>5.Selector选择器</h1><p>我们要从页面中抓取数据，需要解析当前HTML页面<br>Scrapy中自带了选择器，可以选择由XPath或CSS表达式指定的部分<br>在这里我使用XPath进行界面的解析，在爬虫中可以看到<br><code>selector.xpath(&#39;/html/body/div[4]/div[2]/div[2]/div[2]/p[1]/text()&#39;).extract().pop()</code><br><code>.extract_first()</code>是返回内容List中的第一个<br>在爬虫中每次都会创建一个item将抓取到的数据存入item 最后函数会返回一个item</p>
<h1 id="6-Item-pipelines"><a href="#6-Item-pipelines" class="headerlink" title="6.Item pipelines"></a>6.Item pipelines</h1><p>爬虫中返回的Item，都进入到pipelines进行处理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> codecs</div><div class="line"><span class="keyword">import</span> pymongo</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">LianjiaPipeline</span><span class="params">(object)</span>:</span></div><div class="line"></div><div class="line">    collection_name = <span class="string">'lianjia_items'</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></div><div class="line">        self.mongo_uri = mongo_uri</div><div class="line">        self.mongo_db = mongo_db</div><div class="line"></div><div class="line"><span class="meta">    @classmethod</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></div><div class="line">        <span class="keyword">return</span> cls(</div><div class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</div><div class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</div><div class="line">        )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</div><div class="line">        self.db = self.client[self.mongo_db]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></div><div class="line">        self.client.close()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></div><div class="line">        self.db[self.collection_name].insert(dict(item))</div><div class="line">        <span class="keyword">return</span> item</div></pre></td></tr></table></figure></p>
<p>其中必须有<code>process_item</code>来处理爬虫返回的item<br><code>__init__</code>初始化方法<br><code>from_crawler</code>用来读取当前爬虫配置<br><code>open_spider</code>爬虫打开时调用，在此进行数据库链接<br><code>close_spider</code>爬虫关闭时调用，在此关闭数据库链接  </p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在最外层目录外使用命令行执行<code>scrapy crawl lianjia</code><br>其中<code>lianjia</code>是我在爬虫中设定的<code>name</code><br>简单的进行了试验，要关闭Scrapy的<code>ROBOTSTXT_OBEY = False</code><br>今天成功的爬取了2620条数据没有被封ip<br>github:<a href="https://github.com/light-white/lianjia_scrapy" target="_blank" rel="external">lianjia_scrapy</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Scrapy介绍&quot;&gt;&lt;a href=&quot;#1-Scrapy介绍&quot; class=&quot;headerlink&quot; title=&quot;1. Scrapy介绍&quot;&gt;&lt;/a&gt;1. Scrapy介绍&lt;/h1&gt;&lt;p&gt;Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框
    
    </summary>
    
    
      <category term="Python" scheme="http://light-white.me/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://light-white.me/tags/Scrapy/"/>
    
      <category term="MongoDB" scheme="http://light-white.me/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>百度Ueditor图片管理无法显示</title>
    <link href="http://light-white.me/2017/02/17/baidu/"/>
    <id>http://light-white.me/2017/02/17/baidu/</id>
    <published>2017-02-17T07:03:48.000Z</published>
    <updated>2017-02-17T07:28:05.518Z</updated>
    
    <content type="html"><![CDATA[<p>今天在使用ueditor做新闻编辑时出现一点问题，图片上传之后后台管理界面获取不到<br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/ueditor_imagemanage.png" alt=""><br>打开控制台发现获取的是本地绝对路径</p>
<p>第一时间想到修改<code>config.json</code>中的文件访问路径</p>
<pre><code>/* 列出指定目录下的图片 */
&quot;imageManagerActionName&quot;: &quot;listimage&quot;, /* 执行图片管理的action名称 */
&quot;imageManagerListPath&quot;: &quot;/ueditor/jsp/upload/image/&quot;, /* 指定要列出图片的目录 */
&quot;imageManagerListSize&quot;: 20, /* 每次列出文件数量 */
&quot;imageManagerUrlPrefix&quot;: &quot;http://localhost:8080/Meet/&quot;, /* 图片访问路径前缀 */
&quot;imageManagerInsertAlign&quot;: &quot;none&quot;, /* 插入的图片浮动方式 */
&quot;imageManagerAllowFiles&quot;: [&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;, &quot;.gif&quot;, &quot;.bmp&quot;], /* 列出的文件类型 */
</code></pre><p>将UrlPrefix修改为当前项目域名<br>然后发现还有问题  这时候绝对路径跟在域名后面了<br>配置文件<code>ueditor.config.js</code>中说<code>controller.jsp</code>是服务器统一请求接口路径<br>我们修改一下他试试<br>修改后代码如下  </p>
<pre><code>&lt;%@ page language=&quot;java&quot; contentType=&quot;text/html; charset=UTF-8&quot;
    import=&quot;com.baidu.ueditor.ActionEnter&quot;
    pageEncoding=&quot;UTF-8&quot;%&gt;
&lt;%@ page trimDirectiveWhitespaces=&quot;true&quot; %&gt;
&lt;%

    request.setCharacterEncoding( &quot;utf-8&quot; );
    response.setHeader(&quot;Content-Type&quot; , &quot;text/html&quot;);
    String rootPath = application.getRealPath( &quot;/&quot; );
    String action = request.getParameter(&quot;action&quot;);  
    String result = new ActionEnter( request, rootPath ).exec();  
    if( action!=null &amp;&amp;   
       (action.equals(&quot;listfile&quot;) || action.equals(&quot;listimage&quot;) ) ){  
        rootPath = rootPath.replace(&quot;\\&quot;, &quot;/&quot;);  
        result = result.replaceAll(rootPath, &quot;/&quot;);  
    }  
    out.write( result );
%&gt;
</code></pre><p>这时就能看到上传到服务器上的图片与文件了</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天在使用ueditor做新闻编辑时出现一点问题，图片上传之后后台管理界面获取不到&lt;br&gt;&lt;img src=&quot;http://7xtgk5.com1.z0.glb.clouddn.com/ueditor_imagemanage.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;打开控制台发现获
    
    </summary>
    
    
      <category term="ueditor" scheme="http://light-white.me/tags/ueditor/"/>
    
      <category term="html" scheme="http://light-white.me/tags/html/"/>
    
      <category term="jsp" scheme="http://light-white.me/tags/jsp/"/>
    
  </entry>
  
  <entry>
    <title>Windows下Python爬虫框架Scrapy安装</title>
    <link href="http://light-white.me/2017/02/06/Windows%E4%B8%8BPython%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6Scrapy%E5%AE%89%E8%A3%85/"/>
    <id>http://light-white.me/2017/02/06/Windows下Python爬虫框架Scrapy安装/</id>
    <published>2017-02-06T05:57:36.000Z</published>
    <updated>2017-02-06T07:10:39.395Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Scrapy介绍"><a href="#1-Scrapy介绍" class="headerlink" title="1. Scrapy介绍"></a>1. Scrapy介绍</h1><p>Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。<br>Scrapy吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。<br>Scratch，是抓取的意思，这个Python的爬虫框架叫Scrapy，大概也是这个意思吧，就叫它：小刮刮吧。<br>Scrapy的官方网站：<a href="https://scrapy.org/" target="_blank" rel="external">https://scrapy.org/</a>  </p>
<h1 id="2-Python安装"><a href="#2-Python安装" class="headerlink" title="2. Python安装"></a>2. Python安装</h1><p>这步不用说了吧  没有Python怎么用Scrapy<br>Python的官方网站：<a href="https://www.python.org/" target="_blank" rel="external">https://www.python.org/</a><br>我现在使用的是python3.6<br>安装完python记得在控制台输入python,进入python命令行 成功</p>
<h1 id="3-pip包管理器"><a href="#3-pip包管理器" class="headerlink" title="3. pip包管理器"></a>3. pip包管理器</h1><p>pip是python中很好用的包管理器，我们将使用它来安装我们的Scrapy框架<br>pip的下载地址：<a href="https://pypi.python.org/pypi/pip" target="_blank" rel="external">https://pypi.python.org/pypi/pip</a><br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/python_pip.png" alt=""><br>我们下载下面<code>.tar.gz</code>压缩包,下载完成后解压到一个文件夹<br>在控制台下输入  </p>
<pre><code>python setup.py install
</code></pre><p>安装完成后在控制台下输入pip显示出帮助信息 安装完成</p>
<h1 id="4-安装Scrapy"><a href="#4-安装Scrapy" class="headerlink" title="4. 安装Scrapy"></a>4. 安装Scrapy</h1><p>在控制台下输入  </p>
<pre><code>pip install scrapy
</code></pre><p>这时候可能会遇到依赖包安装失败<br>这时候我们在这个网站手动下载依赖包<br><a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/" target="_blank" rel="external">http://www.lfd.uci.edu/~gohlke/pythonlibs/</a><br>例如<code>lxml‑3.7.2‑cp36‑cp36m‑win32.whl</code>中<code>cp36</code>指的是python3.6版本，后面<code>win32</code>指32位程序<br>大家根据自己的python版本进行选择<br>下载到本地之后，使用pip进行安装  </p>
<pre><code>pip install 文件全名
</code></pre><p>例如  </p>
<pre><code>pip install lxml‑3.7.2‑cp36‑cp36m‑win32.whl
</code></pre><h1 id="5-创建Scrapy项目"><a href="#5-创建Scrapy项目" class="headerlink" title="5. 创建Scrapy项目"></a>5. 创建Scrapy项目</h1><p>在控制台中输入  </p>
<pre><code>scrapy startproject 项目名
</code></pre><p>就在当前文件夹下生成scrapy项目</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Scrapy介绍&quot;&gt;&lt;a href=&quot;#1-Scrapy介绍&quot; class=&quot;headerlink&quot; title=&quot;1. Scrapy介绍&quot;&gt;&lt;/a&gt;1. Scrapy介绍&lt;/h1&gt;&lt;p&gt;Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框
    
    </summary>
    
    
      <category term="Windows" scheme="http://light-white.me/tags/Windows/"/>
    
      <category term="Python" scheme="http://light-white.me/tags/Python/"/>
    
      <category term="Scrapy" scheme="http://light-white.me/tags/Scrapy/"/>
    
  </entry>
  
  <entry>
    <title>Windows环境下Mongodb安装及配置</title>
    <link href="http://light-white.me/2016/10/27/Windows%E7%8E%AF%E5%A2%83%E4%B8%8BMongodb%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE/"/>
    <id>http://light-white.me/2016/10/27/Windows环境下Mongodb安装及配置/</id>
    <published>2016-10-27T10:32:32.000Z</published>
    <updated>2016-10-27T10:32:53.380Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-MongoDB介绍"><a href="#1-MongoDB介绍" class="headerlink" title="1. MongoDB介绍"></a>1. MongoDB介绍</h1><p>MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。<br>在高负载的情况下，添加更多的节点，可以保证服务器性能。<br>MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。<br>MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。<br>MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。<br>MongoDB的官方网站： <a href="https://www.mongodb.com/" target="_blank" rel="external">https://www.mongodb.com/</a></p>
<h1 id="2-下载安装包"><a href="#2-下载安装包" class="headerlink" title="2. 下载安装包"></a>2. 下载安装包</h1><p>打开MongoDB官方网站点击Download,下载合适的msi安装包<br>我这里下载的3.2.1版本的<a href="https://www.mongodb.com/download-center?jmp=nav#community" target="_blank" rel="external">mongodb-win32-x86_64-2008plus-ssl-3.2.10-signed.msi</a>  </p>
<h1 id="3-安装MongoDB"><a href="#3-安装MongoDB" class="headerlink" title="3.安装MongoDB"></a>3.安装MongoDB</h1><p>双击安装包，开始安装，同意协议<br>注意这一步，选择这一项<br><img src="http://7xtgk5.com1.z0.glb.clouddn.com//winMongo/Mongodb%20install.png" alt=""><br>点击上面的会直接进入安装<br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/winMongo/Mongodb%20location%20.png" alt=""><br>我们在这里修改MongoDB的安装路径，这里我选择了安装到D盘<br>接下来就直接安装就可以了</p>
<h2 id="3-1配置MongoDB"><a href="#3-1配置MongoDB" class="headerlink" title="3.1配置MongoDB"></a>3.1配置MongoDB</h2><p>安装完成后，我们需要对MongoDB进行一系列的配置<br>最基本的我们要为MongoDB指定数据库的存放路径<br>我在我的MongoDB目录D:\MongoDB下新建一个data目录<br>在data目录下创建db目录作为MongoDB数据库的存放路径<br>创建Logs目录作为日志文件夹<br>在命令行下，我们进入之前安装的MongoDB的bin目录下，执行  </p>
<pre><code>mongod --dbpath d:\MongoDB\data\db  
</code></pre><p>最后显示</p>
<pre><code>2016-10-27T09:23:20.055+0800 I NETWORK  [initandlisten] waiting for connections on port 27017  
</code></pre><p>表示启动成功，我们可以在浏览器输入<a href="http://localhost:27017/" target="_blank" rel="external">http://localhost:27017/</a><br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/winMongo/Mongodb%20port.png" alt=""><br>现在就可以算初步配置完成</p>
<h1 id="4-MongoDB安装Windows服务"><a href="#4-MongoDB安装Windows服务" class="headerlink" title="4.MongoDB安装Windows服务"></a>4.MongoDB安装Windows服务</h1><p>我们要将MongoDB安装为Windows服务，首先要通过管理员权限启动cmd,进入bin目录下，执行</p>
<pre><code>mongod --dbpath &quot;d:\MongoDB\data\db&quot; --logpath &quot;d:\MongoDB\data\logs\MongoDB.log&quot; --logappend --install
</code></pre><p>–dbpath  设置数据库路径<br>–logpath 设置log日志文件<br>–logappend 使用追加的方式写日志  </p>
<p>接下来我们就可以使用</p>
<pre><code>net start MongoDB
</code></pre><p>来启动服务</p>
<p>如果我们想删除MongoDB服务的话，在bin目录下执行</p>
<pre><code>mongod.exe --remove --serviceName &quot;MongoDB&quot;
</code></pre><p>就可以删除MongoDB的服务了</p>
<h1 id="5-编写配置文件"><a href="#5-编写配置文件" class="headerlink" title="5.编写配置文件"></a>5.编写配置文件</h1><p>我们已经可以运行MongoDB服务了<br>但是敲这么长的命令也很麻烦，这时候我们可以编写一个MongoDB的配置文件，使用配置文件来安装MongoDB服务<br>我们继续在\data目录下新建etc目录，在etc目录下创建mongod.conf<br>我门打开mongod.conf  书写配置信息  目前MongoDB配置文件语法使用<a href="http://www.yaml.org/" target="_blank" rel="external">YAML</a></p>
<pre><code>systemLog:
    destination: file
    path: D:\MongoDB\data\logs\MongoDB.log #log日志文件路径
    logAppend : true  #log日志追加模式，不开启这个的话，重启服务会覆盖上次的log文件
net:
    port: 27017 #端口号 默认为27017
storage:
    dbPath: D:\MongoDB\data\db #数据库路径
</code></pre><p>然后使用配置文件启动MongoDB</p>
<pre><code>mongod --config d:\MongoDB\data\etc\mongod.conf
</code></pre><p>打开<a href="http://localhost:27017/" target="_blank" rel="external">http://localhost:27017/</a>查看是否启动成功</p>
<p>详细的配置文件写法请参照<a href="https://docs.mongodb.com/manual/reference/configuration-options/" target="_blank" rel="external">Configuration File Options</a></p>
<p>我门现在可以使用config来创建MongoDB服务，使用管理员权限启动cmd,进入\bin目录</p>
<pre><code>mongod --config d:\MongoDB\data\etc\mongod.conf --install
</code></pre><p>右键我的电脑-&gt;管理-&gt;服务和应用程序-&gt;服务<br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/winMongo/Mongodb%20service.png" alt=""><br>可以看到我们的MongoDB服务已经运行  </p>
<p>到此MongoDB的安装就已经结束，为大家推荐一款MongoDB可视化工具<a href="http://mongobooster.com/" target="_blank" rel="external">mongobooster</a><br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/winMongo/Mongobooster.com.png" alt=""><br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/winMongo/Mongobooster.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-MongoDB介绍&quot;&gt;&lt;a href=&quot;#1-MongoDB介绍&quot; class=&quot;headerlink&quot; title=&quot;1. MongoDB介绍&quot;&gt;&lt;/a&gt;1. MongoDB介绍&lt;/h1&gt;&lt;p&gt;MongoDB 是由C++语言编写的，是一个基于分布式文件存储的
    
    </summary>
    
    
      <category term="Windows" scheme="http://light-white.me/tags/Windows/"/>
    
      <category term="MongoDB" scheme="http://light-white.me/tags/MongoDB/"/>
    
  </entry>
  
  <entry>
    <title>Markdown语法简介</title>
    <link href="http://light-white.me/2016/09/23/Markdown%E8%AF%AD%E6%B3%95%E7%AE%80%E4%BB%8B/"/>
    <id>http://light-white.me/2016/09/23/Markdown语法简介/</id>
    <published>2016-09-23T07:51:04.000Z</published>
    <updated>2017-02-06T07:10:46.440Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Markdown概述"><a href="#1-Markdown概述" class="headerlink" title="1. Markdown概述"></a>1. Markdown概述</h1><p><strong>宗旨</strong></p>
<p>Markdown 的目标是实现「易读易写」。</p>
<p>可读性，无论如何，都是最重要的。一份使用 Markdown 格式撰写的文件应该可以直接以纯文本发布，并且看起来不会像是由许多标签或是格式指令所构成。Markdown 语法受到一些既有 text-to-HTML 格式的影响，包括 Setext、atx、Textile、reStructuredText、Grutatext 和 EtText，而最大灵感来源其实是纯文本电子邮件的格式。</p>
<p>总之， Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像*强调*。Markdown 的列表看起来，嗯，就是列表。Markdown 的区块引用看起来就真的像是引用一段文字，就像你曾在电子邮件中见过的那样。</p>
<h1 id="2-区块元素"><a href="#2-区块元素" class="headerlink" title="2. 区块元素"></a>2. 区块元素</h1><h2 id="2-1-段落和换行"><a href="#2-1-段落和换行" class="headerlink" title="2.1 段落和换行"></a>2.1 段落和换行</h2><p>一个 Markdown 段落是由一个或多个连续的文本行组成，它的前后要有一个以上的空行（空行的定义是显示上看起来像是空的，便会被视为空行。比方说，若某一行只包含空格和制表符，则该行也会被视为空行）。普通段落不该用空格或制表符来缩进。</p>
<p>「由一个或多个连续的文本行组成」这句话其实暗示了 Markdown 允许段落内的强迫换行（插入换行符），这个特性和其他大部分的 text-to-HTML 格式不一样（包括 Movable Type 的「Convert Line Breaks」选项），其它的格式会把每个换行符都转成 <code>&lt;br /&gt;</code> 标签。</p>
<p>如果你确实想要依赖 Markdown 来插入 <code>&lt;br /&gt;</code> 标签的话，在插入处先按入两个以上的空格然后回车。</p>
<p>的确，需要多费点事（多加空格）来产生 <code>&lt;br /&gt;</code> ，但是简单地「每个换行都转换为 <code>&lt;br /&gt;</code>的方法在 Markdown 中并不适合， Markdown 中 email 式的 区块引用 和多段落的 列表 在使用换行来排版的时候，不但更好用，还更方便阅读。</p>
<h2 id="2-2-标题"><a href="#2-2-标题" class="headerlink" title="2.2 标题"></a>2.2 标题</h2><p>Markdown 支持两种标题的语法，类 Setext 和类 atx 形式。</p>
<p>类 Setext 形式是用底线的形式，利用 <code>=</code> （最高阶标题）和 <code>-</code> （第二阶标题），例如：</p>
<pre><code>This is an H1
=============

This is an H2
-------------
</code></pre><p>类 Atx 形式则是在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如：</p>
<pre><code># 这是 H1

## 这是 H2

###### 这是 H6
</code></pre><h2 id="2-3-区块引用-Blockquotes"><a href="#2-3-区块引用-Blockquotes" class="headerlink" title="2.3 区块引用 Blockquotes"></a>2.3 区块引用 Blockquotes</h2><p>Markdown 标记区块引用是使用类似 email 中用 &gt; 的引用方式。如果你还熟悉在 email 信件中的引言部分，你就知道怎么在 Markdown 文件中建立一个区块引用，那会看起来像是你自己先断好行，然后在每行的最前面加上 <code>&gt;</code> ：</p>
<blockquote>
<p>This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet,<br>consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus.<br>Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.</p>
<p>Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse<br>id sem consectetuer libero luctus adipiscing.  </p>
</blockquote>
<p>code:</p>
<pre><code>&gt; This is a blockquote with two paragraphs. Lorem ipsum dolor sit amet,
&gt; consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus.
&gt; Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.
&gt;
&gt; Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse
&gt; id sem consectetuer libero luctus adipiscing.  
</code></pre><p>Markdown 也允许你偷懒只在整个段落的第一行最前面加上 <code>&gt;</code>：<br>区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 <code>&gt;</code> ：</p>
<blockquote>
<p>This is the first level of quoting.</p>
<blockquote>
<p>This is nested blockquote.</p>
</blockquote>
<p>Back to the first level.</p>
</blockquote>
<p>code:</p>
<pre><code>&gt; This is the first level of quoting.
&gt;
&gt; &gt; This is nested blockquote.
&gt;
&gt; Back to the first level.
</code></pre><p>引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等：</p>
<blockquote>
<h2 id="这是一个标题。"><a href="#这是一个标题。" class="headerlink" title="这是一个标题。"></a>这是一个标题。</h2><ol>
<li>这是第一行列表项。</li>
<li>这是第二行列表项。</li>
</ol>
<p>给出一些例子代码：</p>
<pre><code>return shell_exec(&quot;echo $input | $markdown_script&quot;);
</code></pre></blockquote>
<p>code:</p>
<pre><code>&gt; ## 这是一个标题。
&gt;
&gt; 1.   这是第一行列表项。
&gt; 2.   这是第二行列表项。
&gt;
&gt; 给出一些例子代码：
&gt;
&gt;     return shell_exec(&quot;echo $input | $markdown_script&quot;);
</code></pre><h2 id="2-4-列表"><a href="#2-4-列表" class="headerlink" title="2.4 列表"></a>2.4 列表</h2><p>无序列表使用星号、加号或是减号作为列表标记。但是要注意在( * , +, and - )和文字之间需要添加空格。  </p>
<ul>
<li>Red</li>
<li>Green</li>
<li><p>Blue</p>
<pre><code>* Red
* Green
* Blue
</code></pre></li>
</ul>
<p>有序列表则使用数字接着一个英文句点：</p>
<ol>
<li>Bird</li>
<li>McHale</li>
<li><p>Parish</p>
<pre><code>1. Bird
2. McHale
3. Parish
</code></pre></li>
</ol>
<h2 id="2-5-代码区块"><a href="#2-5-代码区块" class="headerlink" title="2.5 代码区块"></a>2.5 代码区块</h2><p>和程序相关的写作或是标签语言原始码通常会有已经排版好的代码区块，通常这些区块我们并不希望它以一般段落文件的方式去排版，而是照原来的样子显示，Markdown 会用 <code>&lt;pre&gt;</code> 和 <code>&lt;code&gt;</code> 标签来把代码区块包起来。</p>
<p>要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：</p>
<pre><code>这是一个普通段落：

    这是一个代码区块。
</code></pre><p>一个代码区块会一直持续到没有缩进的那一行（或是文件结尾）。</p>
<h2 id="2-6-分隔线"><a href="#2-6-分隔线" class="headerlink" title="2.6 分隔线"></a>2.6 分隔线</h2><p>你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：</p>
<pre><code>* * *

***

*****

- - -

---------------------------------------
</code></pre><h1 id="3-区段元素"><a href="#3-区段元素" class="headerlink" title="3 区段元素"></a>3 区段元素</h1><h2 id="3-1-链接"><a href="#3-1-链接" class="headerlink" title="3.1 链接"></a>3.1 链接</h2><p>Markdown 支持两种形式的链接语法： 行内式和参考式两种形式。</p>
<p>不管是哪一种，链接文字都是用 [方括号] 来标记。</p>
<p>要建立一个行内式的链接，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字，只要在网址后面，用双引号把 title 文字包起来即可，例如：</p>
<p>This is <a href="http://example.com/" title="Title" target="_blank" rel="external">an example</a> inline link.<br><a href="http://example.net/" target="_blank" rel="external">This link</a> has no title attribute.</p>
<pre><code>This is [an example](http://example.com/ &quot;Title&quot;) inline link.
[This link](http://example.net/) has no title attribute.
</code></pre><p>链接内容定义的形式为：</p>
<ul>
<li>方括号（前面可以选择性地加上至多三个空格来缩进），里面输入链接文字</li>
<li>接着一个冒号</li>
<li>接着一个以上的空格或制表符</li>
<li>接着链接的网址</li>
<li>选择性地接着 title 内容，可以用单引号、双引号或是括弧包着  </li>
</ul>
<p>链接的定义可以放在文件中的任何一个地方，我比较偏好直接放在链接出现段落的后面，你也可以把它放在文件最后面，就像是注解一样。<br>下面是一个参考式链接的范例：</p>
<p>I get 10 times more traffic from <a href="http://google.com/" title="Google" target="_blank" rel="external">Google</a> than from<br><a href="http://search.yahoo.com/" title="Yahoo Search" target="_blank" rel="external">Yahoo</a> or <a href="http://search.msn.com/" title="MSN Search" target="_blank" rel="external">MSN</a>.</p>
<pre><code>I get 10 times more traffic from [Google] [1] than from
[Yahoo] [2] or [MSN] [3].

  [1]: http://google.com/        &quot;Google&quot;
  [2]: http://search.yahoo.com/  &quot;Yahoo Search&quot;
  [3]: http://search.msn.com/    &quot;MSN Search&quot;
</code></pre><h2 id="3-2-强调"><a href="#3-2-强调" class="headerlink" title="3.2 强调"></a>3.2 强调</h2><p>Markdown 使用星号（<code>*</code>）和底线（<code>_</code>）作为标记强调字词的符号，被 <code>*</code> 或 <code>_</code> 包围的字词会被转成用 <code>&lt;em&gt;</code> 标签包围，用两个 <code>*</code> 或 <code>_</code> 包起来的话，则会被转成<code>&lt;strong&gt;</code>，例如：</p>
<p><em>single asterisks</em></p>
<p><em>single underscores</em></p>
<p><strong>double asterisks</strong></p>
<p><strong>double underscores</strong></p>
<pre><code>*single asterisks*

_single underscores_

**double asterisks**

__double underscores__
</code></pre><p>但是如果你的 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。</p>
<p>如果要在文字前后直接插入普通的星号或底线，你可以用反斜线：<br>*this text is surrounded by literal asterisks*</p>
<pre><code>\*this text is surrounded by literal asterisks\*
</code></pre><h2 id="3-3-代码"><a href="#3-3-代码" class="headerlink" title="3.3 代码"></a>3.3 代码</h2><p>如果要标记一小段行内代码，你可以用反引号把它包起来（`），例如：</p>
<p>（`不是单引号而是左上角的ESC下面~中的`）</p>
<p>Use the <code>printf()</code> function.</p>
<pre><code>Use the `printf()` function.
</code></pre><h2 id="3-4-图片"><a href="#3-4-图片" class="headerlink" title="3.4 图片"></a>3.4 图片</h2><p>很明显地，要在纯文字应用中设计一个「自然」的语法来插入图片是有一定难度的。</p>
<p>Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式。</p>
<p>行内式的图片语法看起来像是：</p>
<pre><code>![Alt text](/path/to/img.jpg)

![Alt text](/path/to/img.jpg &quot;Optional title&quot;)
</code></pre><p>详细叙述如下：</p>
<ul>
<li>一个惊叹号 !</li>
<li>接着一个方括号，里面放上图片的替代文字</li>
<li>接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。</li>
</ul>
<p>参考式的图片语法则长得像这样：</p>
<pre><code>![Alt text][id]
</code></pre><p>「id」是图片参考的名称，图片参考的定义方式则和连结参考一样：</p>
<pre><code>[id]: url/to/image  &quot;Optional title attribute&quot;
</code></pre><h1 id="4-其它"><a href="#4-其它" class="headerlink" title="4. 其它"></a>4. 其它</h1><h2 id="4-1-自动链接"><a href="#4-1-自动链接" class="headerlink" title="4.1 自动链接"></a>4.1 自动链接</h2><p>Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用方括号包起来， Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如：<br><a href="http://light-white.club/" target="_blank" rel="external">http://light-white.club/</a></p>
<pre><code>&lt;http://light-white.club/&gt;
</code></pre><p>邮址的自动链接也很类似</p>
<a href="&#109;&#x61;&#105;&#108;&#x74;&#111;&#58;&#108;&#105;&#103;&#x68;&#x74;&#45;&#119;&#x68;&#105;&#116;&#x65;&#x40;&#111;&#117;&#116;&#x6c;&#x6f;&#x6f;&#107;&#x2e;&#99;&#111;&#109;">&#108;&#105;&#103;&#x68;&#x74;&#45;&#119;&#x68;&#105;&#116;&#x65;&#x40;&#111;&#117;&#116;&#x6c;&#x6f;&#x6f;&#107;&#x2e;&#99;&#111;&#109;</a>

<pre><code>&lt;light-white@outlook.com&gt;
</code></pre><h2 id="4-2-反斜杠"><a href="#4-2-反斜杠" class="headerlink" title="4.2 反斜杠"></a>4.2 反斜杠</h2><p>Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 <code>&lt;em&gt;</code> 标签），你可以在星号的前面加上反斜杠：</p>
<p>*literal asterisks*</p>
<pre><code>\*literal asterisks\*
</code></pre><p>Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号：</p>
<pre><code>\   反斜线
`   反引号
*   星号
_   底线
{}  花括号
[]  方括号
()  括弧
#   井字号
+   加号
-   减号
.   英文句点
!   惊叹号
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Markdown概述&quot;&gt;&lt;a href=&quot;#1-Markdown概述&quot; class=&quot;headerlink&quot; title=&quot;1. Markdown概述&quot;&gt;&lt;/a&gt;1. Markdown概述&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;宗旨&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;M
    
    </summary>
    
    
      <category term="blog" scheme="http://light-white.me/tags/blog/"/>
    
      <category term="Markdown" scheme="http://light-white.me/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>Hexo+Github博客搭建</title>
    <link href="http://light-white.me/2016/04/27/Hexo-Github%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA/"/>
    <id>http://light-white.me/2016/04/27/Hexo-Github博客搭建/</id>
    <published>2016-04-27T08:50:26.000Z</published>
    <updated>2016-09-23T08:06:35.655Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Hexo介绍"><a href="#1-Hexo介绍" class="headerlink" title="1. Hexo介绍"></a>1. Hexo介绍</h1><p>Hexo 是一个简单地、轻量地、基于Node的一个静态博客框架。通过Hexo我们可以快速创建自己的博客，仅需要几条命令就可以完成。<br>发布时，Hexo可以部署在自己的Node服务器上面，也可以部署github上面。对于个人用户来说，部署在github上好处颇多，不仅可以省去服务器的成本，还可以减少各种系统运维的麻烦事(系统管理、备份、网络)。所以，基于github的个人站点，正在开始流行起来….<br>Hexo的官方网站： <a href="https://hexo.io/" target="_blank" rel="external">http://hexo.io/</a>也是基于Github构建的网站。</p>
<h1 id="2-配置环境"><a href="#2-配置环境" class="headerlink" title="2. 配置环境"></a>2. 配置环境</h1><p>安装Node（必须）<br>作用：用来生成静态页面的<br>到Node.js官网下载相应平台的最新版本，一路安装即可。<br>安装Git（必须）<br>作用：把本地的hexo内容提交到github上去.<br>安装sublime 用来编辑文件<br>在Github上new repository<br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/16-4-27/1924680.jpg" alt=""></p>
<p>格式为your_user_name.github.io</p>
<h1 id="3-安装hexo"><a href="#3-安装hexo" class="headerlink" title="3.安装hexo"></a>3.安装hexo</h1><p>打开git shell 利用 npm 命令即可安装<br>npm install -g hexo<br>这里我们可以使用hexo version查看一下版本<br>我现在用的版本是3.2.0 本文也是基于Hexo3.2.0所写<br>后续变更请查看官方文档，Hexo的官方网站： <a href="https://hexo.io/" target="_blank" rel="external">http://hexo.io/</a>  </p>
<h2 id="3-1创建hexo文件夹"><a href="#3-1创建hexo文件夹" class="headerlink" title="3.1创建hexo文件夹"></a>3.1创建hexo文件夹</h2><p>安装完成后，在你喜爱的文件夹下（如D:\hexo）Hexo 即会自动在目标文件夹建立网站所需要的所有文件。<br>hexo init your_user_name.github.io<br>安装好后，我们就可以使用Hexo创建项目了。<br>cd your_user_name.github.io<br>进入目录，并启动Hexo服务器。<br>Hexo server<br>这时端口4000被打开了，我们能过浏览器打开地址，<a href="http://localhost:4000/" target="_blank" rel="external">http://localhost:4000/</a> 。<br><img src="http://7xtgk5.com1.z0.glb.clouddn.com/16-4-27/28198702.jpg" alt=""><br>打开了本地预览模式  </p>
<h1 id="4-全局配置"><a href="#4-全局配置" class="headerlink" title="4.全局配置"></a>4.全局配置</h1><p>_config.yml是全局的配置文件<br>接下来我只写出我们会用到的部分</p>
<pre><code># Hexo Configuration
## Docs: https://hexo.io/docs/configuration.html
## Source: https://github.com/hexojs/hexo/

# Site
title: Hexo#站点名，站点左上角
subtitle: #副标题，站点左上角
description: #站点描述
author: John Doe#作者 站点左下角
language:#语言 中文zh-CN
timezone:#时区

# Deployment
## Docs: https://hexo.io/docs/deployment.html
deploy: #部署配置
  type: git
  repo: https://github.com/your_user_name/ your_user_name.github.io.git
branch: master
</code></pre><p>我们主要需要改的是deploy部分  </p>
<h1 id="5-部署到github"><a href="#5-部署到github" class="headerlink" title="5.部署到github"></a>5.部署到github</h1><p>当我们改好配置文件之后我们就可以把hexo部署到github上了<br>在git shell中进入目录<br>我们需要手动配置一点东西<br>npm install hexo-renderer-ejs–save<br>npm install hexo-renderer-stylus–save<br>npm install hexo-renderer-marked–save<br>npm install hexo-server—save<br>然后生成静态页面<br>hexo g<br>部署到github<br>hexo d  </p>
<h1 id="6-创建新文章"><a href="#6-创建新文章" class="headerlink" title="6.创建新文章"></a>6.创建新文章</h1><p>hexo new 新文章<br>在your_user_name.github.io/source/_posts下会生成 Hexo+Github博客搭建.md<br>我们使用sublime 打开文件，以markdown语法编写</p>
<pre><code>title: 新的开始
date: 2014-05-07 18:44:12
updated    : 2014-05-10 18:44:12
permalink: abc
tags:
- 开始
- 我
- 日记
categories:
- 日志
- 第一天
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-Hexo介绍&quot;&gt;&lt;a href=&quot;#1-Hexo介绍&quot; class=&quot;headerlink&quot; title=&quot;1. Hexo介绍&quot;&gt;&lt;/a&gt;1. Hexo介绍&lt;/h1&gt;&lt;p&gt;Hexo 是一个简单地、轻量地、基于Node的一个静态博客框架。通过Hexo我们可以快速
    
    </summary>
    
    
      <category term="Hexo" scheme="http://light-white.me/tags/Hexo/"/>
    
      <category term="Github" scheme="http://light-white.me/tags/Github/"/>
    
      <category term="blog" scheme="http://light-white.me/tags/blog/"/>
    
  </entry>
  
</feed>
